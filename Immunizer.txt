def gcr(im, percentage): 
'''basic "Gray Component Replacement" function. Returns a CMYK image with 
percentage gray component removed from the CMY channels and put in the 
K channel, ie. for percentage=100, (41, 100, 255, 0) >> (0, 59, 214, 41)''' 
cmyk_im = im.convert('CMYK')
if not percentage: 
return cmyk_im 
cmyk_im = cmyk_im.split() 
cmyk = [] 
for i in range(4): 
cmyk.append(cmyk_im[i].load()) 
for x in range(im.size[0]): 
for y in range(im.size[1]): 
gray = min(cmyk[0][x,y], cmyk[1][x,y], cmyk[2][x,y]) * percentage / 100 
for i in range (3): 
cmyk[i][x,y] = cmyk[i][x,y] - gray 
cmyk[3][x,y] = gray 
return Image.merge('CMYK', cmyk_im) 
def halftone(im, cmyk, sample, scale): 
'''Returns list of half-tone images for cmyk image. sample (pixels), 
determines the sample box size from the original image. The maximum 
output dot diameter is given by sample * scale (which is also the number 
of possible dot sizes). So sample=1 will presevere the original image 
resolution, but scale must be >1 to allow variation in dot size.''' 
cmyk = cmyk.split() 
dots = [] 
angle = 0 
for channel in cmyk: 
channel = channel.rotate(angle, expand=1) 
size = channel.size[0]*scale, channel.size[1]*scale 
half_tone = Image.new('L', size) 
draw = ImageDraw.Draw(half_tone) 
for x in range(0, channel.size[0], sample): 
for y in range(0, channel.size[1], sample): 
box = channel.crop((x, y, x + sample, y + sample)) 
stat = ImageStat.Stat(box) 
diameter = (stat.mean[0] / 255)**0.5 
edge = 0.5*(1-diameter) 
x_pos, y_pos = (x+edge)*scale, (y+edge)*scale
box_edge = sample*diameter*scale 
draw.ellipse((x_pos, y_pos, x_pos + box_edge, y_pos + box_edge), fill=255) 
half_tone = half_tone.rotate(-angle, expand=1) 
width_half, height_half = half_tone.size 
xx=(width_half-im.size[0]*scale) / 2 
yy=(height_half-im.size[1]*scale) / 2 
half_tone = half_tone.crop((xx, yy, xx + im.size[0]*scale, yy + im.size[1]*scale)) 
dots.append(half_tone) 
angle += 15 
return dots 
def compare(img1,img2): 
# Load images 
before = cv2.imread("static/upload/"+img1) 
after = cv2.imread("static/upload/"+img2) 
# Convert images to grayscale 
before_gray = cv2.cvtColor(before, cv2.COLOR_BGR2GRAY) 
after_gray = cv2.cvtColor(after, cv2.COLOR_BGR2GRAY) 
# Compute SSIM between the two images 
(score, diff) = structural_similarity(before_gray, after_gray, full=True) 
print ("Image Similarity: {:.4f} %”. format (score * 100)) 
per=format (score * 100) 
# The diff image contains the actual image differences between the two images 
# and is represented as a floating point data type in the range [0,1] 
# so we must convert the array to 8-bit unsigned integers in the range 
# [0,255] before we can use it with OpenCV 
diff = (diff * 255).astype("uint8") 
diff_box = cv2.merge([diff, diff, diff]) 
# Threshold the difference image, followed by finding contours to 
# obtain the regions of the two input images that differ 
thresh = cv2.threshold(diff, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU) [1] 
contours = cv2.findContours(thresh, cv2.RETR_EXTERNAL, 
cv2.CHAIN_APPROX_SIMPLE) 
contours = contours [0] if len(contours) == 2 else contours [1] 
mask = np.zeros(before.shape, dtype='uint8') 
filled_after = after.copy() 
j=1 
for c in contours: 
area = cv2.contourArea(c) 
if area > 40: 
x,y,w,h = cv2.boundingRect(c) 
cv2.rectangle(before, (x, y), (x + w, y + h), (36,255,12), 2) 
mm=cv2.rectangle(after, (x, y), (x + w, y + h), (36,255,12), 2) 
cv2.imwrite("static/test/ggg.jpg", mm) 
image = cv2.imread("static/test/ggg.jpg") 
cropped = image [y:y+h, x:x+w] 
gg="f"+str(j)+".jpg" 
cv2.imwrite("static/test/"+gg, cropped) 
cv2.rectangle(diff_box, (x, y), (x + w, y + h), (36,255,12), 2) 
cv2.drawContours(mask, [c], 0, (255,255,255), -1) 
cv2.drawContours(filled_after, [c], 0, (0,255,0), -1) 
j+=1 
value=[per,j] 
return value 
def attack1(img1,img2): 
# Load images 
before = cv2.imread("static/upload/"+img2) 
after = cv2.imread("static/upload/"+img1) 
# Convert images to grayscale 
before_gray = cv2.cvtColor(before, cv2.COLOR_BGR2GRAY) 
after_gray = cv2.cvtColor(after, cv2.COLOR_BGR2GRAY) 
# Compute SSIM between the two images 
(score, diff) = structural_similarity(before_gray, after_gray, full=True) 
print("Image Similarity: {:.4f}%".format(score * 100)) 
per=format(score * 100) 
diff = (diff * 255).astype("uint8") 
diff_box = cv2.merge([diff, diff, diff]) 
# obtain the regions of the two input images that differ 
thresh = cv2.threshold(diff, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1] 
contours = cv2.findContours(thresh, cv2.RETR_EXTERNAL, 
cv2.CHAIN_APPROX_SIMPLE) 
contours = contours[0] if len(contours) == 2 else contours[1] 
mask = np.zeros(before.shape, dtype='uint8') 
filled_after = after.copy() 
j=1 
for c in contours: 
area = cv2.contourArea(c) 
if area > 40: 
x,y,w,h = cv2.boundingRect(c) 
cv2.rectangle(before, (x, y), (x + w, y + h), (36,255,12), 2) 
mm=cv2.rectangle(after, (x, y), (x + w, y + h), (36,255,12), 2) 
cv2.imwrite("static/test/ggg.jpg", mm) 
image = cv2.imread("static/test/ggg.jpg") 
cropped = image [y:y+h, x:x+w] 
gg="h"+str(j)+".jpg" 
cv2.imwrite("static/test/"+gg, cropped) 
cv2.rectangle(diff_box, (x, y), (x + w, y + h), (36,255,12), 2) 
cv2.drawContours(mask, [c], 0, (255,255,255), -1) 
cv2.drawContours(filled_after, [c], 0, (0,255,0), -1) 
j+=1 
e=j-1 
n=1 
y=0 
k=0 
while n<=e: 
main_image = cv2.imread('static/upload/'+img1) 
gray_image = cv2.cvtColor(main_image, cv2.COLOR_BGR2GRAY) 
ffn="h"+str(n)+".jpg" 
template = cv2.imread("static/test/"+ffn, 0) 
width, height = template.shape[::-1] #get the width and height 
match = cv2.matchTemplate(gray_image, template, cv2.TM_CCOEFF_NORMED) 
threshold = 0.8 
position = np.where(match >= threshold) #get the location of template in the image
k=0 
cv2.rectangle(main_image, point, (point[0] + width, point[1] + height), (0, 204, 153), 0) 
k+=1 
if k>1: 
y+=k 
n+=1 
cv2.imshow('before', before) 
cv2.imshow('after', after) 
cv2.imshow('diff', diff) 
cv2.imshow('diff_box', diff_box) 
cv2.imshow('mask', mask) 
cv2.imshow('filled after', filled_after) 
value=[per,j,y] 
return value